{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{97: {98: {}}, 98: {}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Tokenization:\n",
    "    def __init__(self, seed=None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self.tree = {}\n",
    "        self.encode_map = {b'': 0}\n",
    "        self.decode_map = {0: b''}\n",
    "\n",
    "    def add(self, text: bytes):\n",
    "        assert text, repr(text)\n",
    "        current = self.tree\n",
    "        # Add all substrings to the tree before the full string\n",
    "        if text[:-1] not in self.encode_map:\n",
    "            self.add(text[:-1])\n",
    "        # Add new token to our maps\n",
    "        token = len(self.encode_map)\n",
    "        self.encode_map[text] = token\n",
    "        self.decode_map[token] = text\n",
    "        # Traverse the tree to add the node\n",
    "        while text:\n",
    "            t, *text = text\n",
    "            if t not in current:\n",
    "                current[t] = {}\n",
    "            current = current[t]\n",
    "\n",
    "    def encode_step(self, text: bytes, compression: float=0.9):\n",
    "        ''' Encode a single step, returning the next token and remaining text '''\n",
    "        assert 0.0 <= compression <= 1.0, repr(compression)\n",
    "        # Traverse the tree\n",
    "        code = []\n",
    "        current = self.tree\n",
    "        while text and text[0] in current and self.rng.random() < compression:\n",
    "            code.append(text[0])\n",
    "            current = current[text[0]]\n",
    "            text = text[1:]\n",
    "        # Return the token and remaining text\n",
    "        return self.encode_map(bytes(code)), text\n",
    "\n",
    "    def encode(self, text: bytes, compression: float=0.9):\n",
    "        ''' Encode a string, returning a list of tokens '''\n",
    "        assert 0.0 <= compression <= 1.0, repr(compression)\n",
    "        # Encode the text step by step\n",
    "        tokens = []\n",
    "        while len(text):\n",
    "            token, text = self.encode_step(text, compression=compression)\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list):\n",
    "        ''' Decode a list of tokens, returning the original text '''\n",
    "        return b''.join(self.decode_map[t] for t in tokens)\n",
    "\n",
    "\n",
    "tok = Tokenization()\n",
    "tok.add(b'a')\n",
    "tok.add(b'b')\n",
    "tok.add(b'ab')\n",
    "tok.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code b'aa' False\n",
      "testing b'a' True\n",
      "testing b'aa' False\n",
      "adding b'aa'\n",
      "code b'aaa' False\n",
      "testing b'a' True\n",
      "testing b'aa' True\n",
      "testing b'aaa' False\n",
      "adding b'aaa'\n",
      "code b'aaab' False\n",
      "testing b'a' True\n",
      "testing b'aa' True\n",
      "testing b'aaa' True\n",
      "testing b'aaab' False\n",
      "adding b'aaab'\n"
     ]
    }
   ],
   "source": [
    "from infoseq.token import Tokenization\n",
    "from infoseq.bpe import get_pairs, bpe_from_text\n",
    "\n",
    "\n",
    "text = b'aaabdaaabac'\n",
    "max_tokens=257 + 3\n",
    "compression=1.0\n",
    "seq_len=len(text)\n",
    "num_seq=1\n",
    "seed=0\n",
    "tok = Tokenization.basic(0)\n",
    "while len(tok) < max_tokens:\n",
    "    # Get pairs until we get something new\n",
    "    for code in get_pairs(text, tok, compression=compression, seq_len=seq_len, num_seq=num_seq, seed=seed):\n",
    "        print('code', code, code in tok)\n",
    "        if code not in tok:\n",
    "            break\n",
    "    else:\n",
    "        assert False, \"Could not find new code\"\n",
    "    # Add the smallest sub-code which is new\n",
    "    for i in range(1, len(code) + 1):\n",
    "        print('testing', code[:i], code[:i] in tok)\n",
    "        if code[:i] not in tok:\n",
    "            print('adding', code[:i])\n",
    "            tok.add(code[:i])\n",
    "            assert code[:i] in tok\n",
    "            break\n",
    "    else:\n",
    "        assert False, f\"{code} ({len(code)}) : {code in tok}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98e383efd562c29a75ad81f9b3092a5b6976e9c917db4b8c3a6ed0ebbc0b47f0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
